{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Feb 21 2021\n",
    "@author: Sahand-j\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader,datetime\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date \n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import yahoo_fin.stock_info as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = 'tsla,amat,avgo,voo,vti,jpm,iipr,vt,vxus,tgt,dfs,schd,dgro,nobl,schb,spy,nsc,sdy,gm,unp,qqq,dis,land,aapl,stor,ko'\n",
    "s = 'tsla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_prices_df(ticker_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inserts each stock ticker price history into database\n",
    "    :@return(string): returns string confirming query execution\n",
    "    \"\"\"\n",
    "    \n",
    "    engine = create_engine('postgresql://postgres:postgres@localhost:5432/Stocks')\n",
    "    \n",
    "    for i in ticker_list.split(','):\n",
    "        df = si.get_data(i)\n",
    "    \n",
    "        # Basic formatting\n",
    "        df = df[['ticker','open', 'high', 'low', 'close', 'adjclose', 'volume']]    \n",
    "        df = df.fillna(0)\n",
    "        df['updated'] = pd.to_datetime('now')\n",
    "        \n",
    "        # Resetting index for db\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.rename(columns = {'index':'date'})\n",
    "\n",
    "        \n",
    "        # Write the data into the database\n",
    "        check = engine.has_table('stocks_daily_prices')\n",
    "        print(check)\n",
    "        if(check == False):\n",
    "            df.to_sql('stocks_daily_prices', engine, if_exists='replace')\n",
    "\n",
    "\n",
    "    '''# Create a primary key on the table\n",
    "    query = \"\"\"ALTER TABLE stocks_daily_prices \n",
    "                ADD PRIMARY KEY (ticker,date);\"\"\"\n",
    "    engine.execute(query)'''\n",
    "\n",
    "    return 'query succesful'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis data \n",
    "\"\"\"\n",
    "    Inserts each stock ticker price history into database\n",
    "    :@return(string): returns string confirming query execution\n",
    "    \"\"\"\n",
    "\n",
    "def stock_news_headline_scraper(ticker_list):\n",
    "    \n",
    "    website_url = 'https://finviz.com/quote.ashx?t='\n",
    "    news_tables = {}\n",
    "    engine = create_engine('postgresql://postgres:postgres@localhost:5432/Stocks')\n",
    "\n",
    "    for ticker in ticker_list.split(','):\n",
    "\n",
    "        #URL for each stock\n",
    "        url = website_url + ticker\n",
    "\n",
    "        #requesting url for each ticker\n",
    "        response = urlopen(Request(url=url, headers={'user-agent': 'sentiment-analysis-app'}))\n",
    "\n",
    "        #html parser, using bs4. downloaded the html\n",
    "        html = BeautifulSoup(response,'html')\n",
    "\n",
    "        #the body that contains all the news article links\n",
    "        news_table_html_body = html.find(id = 'news-table')\n",
    "\n",
    "        #each stock is in dictionary with value corresponding to news table\n",
    "        news_tables.update({ ticker.upper() : news_table_html_body })\n",
    "\n",
    "\n",
    "    parsed_data = []\n",
    "\n",
    "    #itterating over key and value pairs. itterating over a dict\n",
    "    for ticker, news_tables in news_tables.items():\n",
    "\n",
    "        # news_tables.find_all('tr') is bs4 list of all articles headlins\n",
    "        for row in news_tables.find_all('tr'):\n",
    "\n",
    "            #title is in acnchor tag 'a', retrieving that from bs4 obj row\n",
    "            title = row.a.text\n",
    "\n",
    "            #time stamps have td tags\n",
    "            timestamp = row.td.text\n",
    "\n",
    "            #no date information\n",
    "            if(len(timestamp.split(' ')) == 1):\n",
    "                time = timestamp.split(' ')[0]\n",
    "\n",
    "            #has date info, before time\n",
    "            else:\n",
    "                date = timestamp.split(' ')[0]\n",
    "                time = timestamp.split(' ')[1]\n",
    "\n",
    "            parsed_data.append([ticker,title,date,time])\n",
    "        \n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(parsed_data,columns=['ticker','title','date','time'])\n",
    "        \n",
    "        #compund score for each article title\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        df['comp_score'] = df['title'].apply(lambda title : vader.polarity_scores(title)['compound'])\n",
    "        \n",
    "        for i in df.index:\n",
    "            df.at[i, 'time'] = df['time'][i][0:7]\n",
    "        \n",
    "        #converting string time col to datetime obj   \n",
    "        df['time'] = pd.to_datetime(df['time']).dt.strftime('%H:%M:%S')\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        \n",
    "        #df = df.set_index('date')\n",
    "        df['updated'] = pd.to_datetime('now')\n",
    "        \n",
    "        #filterign nuetral news out of df\n",
    "        df = df[df.comp_score != 0]\n",
    "\n",
    "        df = df[['date','ticker','comp_score','title','updated']]\n",
    "        \n",
    "        \n",
    "        # Write the data into the database\n",
    "        check = engine.has_table('stock_sentiments')\n",
    "        print(check)\n",
    "        if check == False:\n",
    "            # Write the data into the database\n",
    "            df.to_sql('stock_sentiments', engine, if_exists='replace')\n",
    "        \n",
    "        \n",
    "    '''# Create a primary key on the table\n",
    "    query = \"\"\"ALTER TABLE stock_sentiments \n",
    "                ADD PRIMARY KEY (ticker, date, time);\"\"\"\n",
    "    \n",
    "    engine.execute(query) \n",
    "    #return df   '''\n",
    "    \n",
    "    return 'sentiment table created successfully'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'query succesful'"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices_df(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentiment table created successfully'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_news_headline_scraper(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "#happens second\n",
    "merged_price_sent_view = '''create view joined_sentiment_stock_price as\n",
    "select grouped_sentiment.date, grouped_sentiment.avg, stocks_daily_prices.ticker,\n",
    "stocks_daily_prices.adjclose,stocks_daily_prices.volume,stocks_daily_prices.high,stocks_daily_prices.low\n",
    "from grouped_sentiment\n",
    "join stocks_daily_prices\n",
    "on grouped_sentiment.date = stocks_daily_prices.date\n",
    "and grouped_sentiment.ticker = stocks_daily_prices.ticker\n",
    "order by grouped_sentiment.date desc;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#happens first\n",
    "avg_comp_per_day_view = '''create view grouped_sentiment as\n",
    "select date, ticker, avg(comp_score)\n",
    "from stock_sentiments\n",
    "group by date,ticker\n",
    "order by date desc;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/Stocks')\n",
    "\n",
    "check1 = engine.has_table('joined_sentiment_stock_price')\n",
    "check2 = engine.has_table('grouped_sentiment')\n",
    "\n",
    "print(check1)\n",
    "if check1 == False:\n",
    "    engine.execute(avg_comp_per_day_view)\n",
    "print(check2)\n",
    "if check2 == False:\n",
    "    engine.execute(merged_price_sent_view)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TSLA':       index       date ticker        open        high         low       close  \\\n",
       " 0         0 2010-06-29   TSLA    3.800000    5.000000    3.508000    4.778000   \n",
       " 1         1 2010-06-30   TSLA    5.158000    6.084000    4.660000    4.766000   \n",
       " 2         2 2010-07-01   TSLA    5.000000    5.184000    4.054000    4.392000   \n",
       " 3         3 2010-07-02   TSLA    4.600000    4.620000    3.742000    3.840000   \n",
       " 4         4 2010-07-06   TSLA    4.000000    4.000000    3.166000    3.222000   \n",
       " ...     ...        ...    ...         ...         ...         ...         ...   \n",
       " 2676   2676 2021-02-16   TSLA  818.000000  821.000000  792.440002  796.219971   \n",
       " 2677   2677 2021-02-17   TSLA  779.090027  799.840027  762.010010  798.150024   \n",
       " 2678   2678 2021-02-18   TSLA  780.900024  794.690002  776.270020  787.380005   \n",
       " 2679   2679 2021-02-19   TSLA  795.000000  796.789978  777.369995  781.299988   \n",
       " 2680   2680 2021-02-22   TSLA  762.640015  768.500000  710.500000  714.500000   \n",
       " \n",
       "         adjclose    volume                    updated  rolling_mean  \\\n",
       " 0       4.778000  93831500 2021-02-23 00:44:35.959031           NaN   \n",
       " 1       4.766000  85935500 2021-02-23 00:44:35.959031           NaN   \n",
       " 2       4.392000  41094000 2021-02-23 00:44:35.959031           NaN   \n",
       " 3       3.840000  25699000 2021-02-23 00:44:35.959031           NaN   \n",
       " 4       3.222000  34334500 2021-02-23 00:44:35.959031           NaN   \n",
       " ...          ...       ...                        ...           ...   \n",
       " 2676  796.219971  19686700 2021-02-23 00:44:35.959031    261.386601   \n",
       " 2677  798.150024  25996500 2021-02-23 00:44:35.959031    263.245930   \n",
       " 2678  787.380005  17897000 2021-02-23 00:44:35.959031    265.079024   \n",
       " 2679  781.299988  18904800 2021-02-23 00:44:35.959031    266.895761   \n",
       " 2680  714.500000  35628285 2021-02-23 00:44:35.959031    268.546985   \n",
       " \n",
       "       rolling_std  cumel_return  \n",
       " 0             NaN           NaN  \n",
       " 1             NaN      0.997488  \n",
       " 2             NaN      0.919213  \n",
       " 3             NaN      0.803684  \n",
       " 4             NaN      0.674341  \n",
       " ...           ...           ...  \n",
       " 2676   235.380818    166.642945  \n",
       " 2677   236.661065    167.046891  \n",
       " 2678   237.859779    164.792806  \n",
       " 2679   239.006686    163.520303  \n",
       " 2680   239.801034    149.539560  \n",
       " \n",
       " [2681 rows x 13 columns]}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting db portfolio data based on stocks\n",
    "#we have a dcitionary of tickers and their stock data with rolling means, cumel ret, \n",
    "#and std to be analyzed ith sentiment\n",
    "\n",
    "stock_price_cent_df = pd.read_sql_query('select * from joined_sentiment_stock_price;', engine)\n",
    "daily_price_info_df = pd.read_sql_query('select * from stocks_daily_prices;', engine)\n",
    "\n",
    "dict_of_dfs = {}\n",
    "for i in s.split(','):\n",
    "    temp_df = daily_price_info_df[daily_price_info_df['ticker'] == i.upper()]\n",
    "    \n",
    "    temp_df['rolling_mean'] = temp_df['adjclose'].rolling(round(len(temp_df)*.15)).mean()\n",
    "    temp_df['rolling_std'] = temp_df['adjclose'].rolling(round(len(temp_df)*.15)).std()\n",
    "    temp_df['cumel_return'] = (1 + temp_df['adjclose'].pct_change(1)).cumprod()\n",
    "\n",
    "    dict_of_dfs.update({i.upper() : temp_df})\n",
    "\n",
    "#list_of_dfs.get('TSLA')\n",
    "dict_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily_price_info_df[daily_price_info_df['ticker']=='TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new table with new stock info\n",
    "daily_price_info_df.fillna(0).to_sql('stock_price_calc', engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''corrMatrix = tsla.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
